{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f6c3a9d",
   "metadata": {},
   "source": [
    "# NLP Libraries\n",
    "When you building an NLP application, you might find out using multiple libraries such as **SpaCy, NLTK, Gensim, Pytorch, Tensorflow Hugging Face** and so on. **SpaCy and NLTK** are two most popular libraries to be used in any NLP application and they tend to do similar things.\n",
    "\n",
    "So the main differences between **SpaCy** and **NLTK** are listed bellow:\n",
    "\n",
    "**1.** SpaCy is an object oriented but NLTK is string processing library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed86b4c",
   "metadata": {},
   "source": [
    "<img src = \"img.jpg\" width = \"600px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e4279",
   "metadata": {},
   "source": [
    "**2.** Spacy provide most efficient NLP algorithm for a given task. NLTK provides to many algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5f3a16",
   "metadata": {},
   "source": [
    "<img src = \"img1.jpg\" width = \"600px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44557a6d",
   "metadata": {},
   "source": [
    "**3.** Spacy is more user friendly than NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823ec04",
   "metadata": {},
   "source": [
    "<img src = \"img2.jpg\" width = \"600px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b6918",
   "metadata": {},
   "source": [
    "**4.** Spacy doesn't allow you to run a specifica algorithm but NLTK allow you to run many algorithms. For example when we do word tokenization by five ways, Spacy doesn't allow you to use all the five ways, its own select the best way for you. NLTK allow you to try different algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de3752",
   "metadata": {},
   "source": [
    "<img src = \"img3.png\" width = \"600px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae436d3b",
   "metadata": {},
   "source": [
    "**5.** Spacy is new library & has active user community. But NLTK is old library and it's user community is not active as Spacy user community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c70d1a1",
   "metadata": {},
   "source": [
    "<img src = \"img4.jpg\" width = \"600px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a68733d",
   "metadata": {},
   "source": [
    "The total differences are listed bellow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716a095",
   "metadata": {},
   "source": [
    "<img src = \"img5.jpg\" width = \"600px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5abbe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So to clear the differences, here we do sentence and word tokenization using spacy library.\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bec02a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Ahmad loves going to beach.\n",
      "Ali loves to to go swimming.\n"
     ]
    }
   ],
   "source": [
    "# So here using spacy load function which used to load specific packages, we want to load the English core package \n",
    "# 'en_core_web_sm'.\n",
    "# After NLP object creation we create a document 'doc' and supply a text which has two sentences.\n",
    "# Then we'll print the sentences.\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Dr. Ahmad loves going to beach. Ali loves to to go swimming.\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)          # So this is basically sentence tokenization in spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeee030d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Ahmad\n",
      "loves\n",
      "going\n",
      "to\n",
      "beach\n",
      ".\n",
      "Ali\n",
      "loves\n",
      "to\n",
      "to\n",
      "go\n",
      "swimming\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Now if we want to separate individual words from these sentences which is called word tokenization, we do simply as \n",
    "# bellow:\n",
    "for sentence in doc.sents:\n",
    "    for word in sentence:\n",
    "        print(word)          # So this is basically word tokenization in spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d889ba0e",
   "metadata": {},
   "source": [
    "* **So by looking upper sentence and word tokenization in spacy is in form of object oriented.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3beb03d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Habib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's do the same thing in NLTK.\n",
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df4fe9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In NLTK we have many tokenizer which we can choose.\n",
    "\n",
    "# from nltk import ABCMeta\n",
    "# from nltk import AbstractCollocationFinder\n",
    "# from nltk import AbstractLazySequence\n",
    "# ...\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fa033f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr. Ahmad loves going to beach.', 'Ali loves to to go swimming.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So now we'll pass the string to the 'sent_tokenize':\n",
    "sent_tokenize(\"Dr. Ahmad loves going to beach. Ali loves to to go swimming.\")\n",
    "# So we see here is no object, that's why NLTK is not object oriented. We give string as input and it give sting as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bf4d2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'Ahmad',\n",
       " 'loves',\n",
       " 'going',\n",
       " 'to',\n",
       " 'beach',\n",
       " '.',\n",
       " 'Ali',\n",
       " 'loves',\n",
       " 'to',\n",
       " 'to',\n",
       " 'go',\n",
       " 'swimming',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar for word tokenization:\n",
    "from nltk import word_tokenize\n",
    "word_tokenize(\"Dr. Ahmad loves going to beach. Ali loves to to go swimming.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
