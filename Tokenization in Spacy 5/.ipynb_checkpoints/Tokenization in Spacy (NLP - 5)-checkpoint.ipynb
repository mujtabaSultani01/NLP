{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13652f1c",
   "metadata": {},
   "source": [
    "## Tokenization in Spacy\n",
    "In Spacy library, tokenization refers to the process of breaking down a text into individual units called tokens. These tokens are typically words, punctuation marks, or other meaningful elements of the text. The tokenization process involves identifying word boundaries and separating them from other elements of the text such as numbers, symbols, and whitespace. This is an important step in NLP tasks as it allows for further analysis and processing of the text data. Spacy uses advanced tokenization algorithms that can handle complex text structures such as compound words, contractions, and hyphenated words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9c5dd",
   "metadata": {},
   "source": [
    "According to NLP pipeline which is discussed in previouse notebook & shown in image bellow, we'll discuss sentence and word tokenization in **Spacy library.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6712d5",
   "metadata": {},
   "source": [
    "<img src = \"img.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231faa1",
   "metadata": {},
   "source": [
    "**Tokenization is a process of splitting text into meaningful segments.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38c99f",
   "metadata": {},
   "source": [
    "<img src = \"img1.jpg\" width = \"800px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df88ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's import 'Spacy':\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87628e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we want to create a language object. In spacy we can create object in different ways. One way is to create blank \n",
    "# object for English language 'en'.\n",
    "nlp = spacy.blank(\"en\")   # this just understand english. if you're using other language, then you can search for other\n",
    "                          # language modesl. (spacy language models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e5cb3c",
   "metadata": {},
   "source": [
    "<img src = \"img2.jpg\" width = \"600px\" height = \"200px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35f01b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "# Next we create a document and here we provide our text. Our text could be a paragraph, or multi page documents...\n",
    "# Next we simple create text tokens.\n",
    "doc = nlp('''\"let's go to N.Y.!\"''')\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414977be",
   "metadata": {},
   "source": [
    "**The process is perform as follow:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b876fa2",
   "metadata": {},
   "source": [
    "<img src = \"img3.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a940071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Faizan\n",
      "visited\n",
      "the\n",
      "hospotal\n",
      "and\n",
      "he\n",
      "ordered\n",
      "the\n",
      "necessary\n",
      "tools\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Let's pass some other text.\n",
    "doc = nlp(\"Dr. Faizan visited the hospotal and he ordered the necessary tools.\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f91dd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can choose each token individually:\n",
    "doc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3bcf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dr. Faizan visited the"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or\n",
    "doc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "598240ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to check the object type, it should be an english language object. \n",
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df17764e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarly 'doc' will be an object of document:\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90c574c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token:\n",
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34a2126a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "let"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1 = doc[1]\n",
    "token1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5552fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now if you do 'dir' for python variable, it will shows you all the methods of that class:\n",
    "token1 = doc[1]\n",
    "dir(token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79a548ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So let's try to use some of these methods.\n",
    "# \n",
    "token1.is_alpha   # It will answer you 'True' because it's simply alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3d35142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67e93d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's have another text:\n",
    "doc = nlp(\"She gives two $ to her brother.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56317563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = doc[2]\n",
    "token2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "218fd8a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.like_num # Will answer you 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0f1f67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3 = doc[3]\n",
    "token3.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a41de91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e3acbef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She ==> index:  0 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "gives ==> index:  1 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "two ==> index:  2 is_alpha: True is_punct: False like_num: True is_currency: False\n",
      "$ ==> index:  3 is_alpha: False is_punct: False like_num: False is_currency: True\n",
      "to ==> index:  4 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "her ==> index:  5 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      "brother ==> index:  6 is_alpha: True is_punct: False like_num: False is_currency: False\n",
      ". ==> index:  7 is_alpha: False is_punct: True like_num: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "# Let's print some tokens using loop, 'token.i' call for index.\n",
    "for token in doc:\n",
    "    print(token, \"==>\", \"index: \", token.i,\n",
    "          \"is_alpha:\", token.is_alpha, \n",
    "          \"is_punct:\", token.is_punct, \n",
    "          \"like_num:\", token.like_num,\n",
    "          \"is_currency:\", token.is_currency,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983db86",
   "metadata": {},
   "source": [
    "### Collecting email ids of students from students information sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be0163c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we read the file:\n",
    "with open(\"students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text   # It will return the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dd18adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n \\n \\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to convert the array into a simple text:\n",
    "text = \" \".join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58621e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virat@kohli.com',\n",
       " 'maria@sharapova.com',\n",
       " 'serena@williams.com',\n",
       " 'joe@root.com']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next we can find emails from the text using 'like_emails' function:\n",
    "doc = nlp(text)\n",
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "emails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5a368d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ممنون False False\n",
      "از False False\n",
      "لطف False False\n",
      "شما False False\n",
      "، False False\n",
      "مبلغ False False\n",
      "6000 False True\n",
      "$ True False\n",
      "به False False\n",
      "حساب False False\n",
      "شما False False\n",
      "اضافه False False\n",
      "شد False False\n",
      ". False False\n"
     ]
    }
   ],
   "source": [
    "# You can use different language models. Here let's see the Persian example:\n",
    "nlp = spacy.blank(\"fa\")\n",
    "doc = nlp(\"ممنون از لطف شما، مبلغ 6000$ به حساب شما اضافه شد.\")\n",
    "for token in doc:\n",
    "    print(token, token.is_currency, token.like_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac5ca8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sometimes you want to customize your tokenizer. I means when you see a special keyword in text, instead of that keyword \n",
    "# return me the splitted form of that keyword.\n",
    "\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2337f621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "    {ORTH: \"gim\"},\n",
    "    {ORTH: \"me\"},\n",
    "])\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens    # In tokenization we can't change the actual things, for example to return 'give' and 'me' instead of 'gim' and\n",
    "          # 'me'. We'll do that in comming notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cae0c5d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# The next topic is with blank tokenizer we can just take tokenizer not other things.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# For example now if we want to split a text into sentences, it will give us an error.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sentence)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokens\\doc.pyx:892\u001b[0m, in \u001b[0;36msents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "# The next topic is with blank tokenizer we can just take tokenizer not other things.\n",
    "# For example now if we want to split a text into sentences, it will give us an error.\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f91e0a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So the error says that the pipeline is empty:\n",
    "nlp.pipe_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cd8609e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1cccaf0c840>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now if we add sentence tokenizer the problem will be solved.\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7b782fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now if we check the pipeline, the sentenceizer will be added:\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab3a3a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi.\n"
     ]
    }
   ],
   "source": [
    "# So now we can split the text into sentences:\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi.\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b6c24f",
   "metadata": {},
   "source": [
    "* The sentencizer which we added is not able to know full features of English language, that's why its outputed the whole text as single sentence. But if we use 'nlp = spacy.load(\"en_core_web_sm\")' pipeline, this issue will be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6cc87288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji of mumbai.\n",
      "Hulk loves chat of delhi.\n"
     ]
    }
   ],
   "source": [
    "# Let's see the 'nlp = spacy.load(\"en_core_web_sm\")' pipeline:\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi.\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)            # This will work fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca511918",
   "metadata": {},
   "source": [
    "* So the basic idea is, if we use **blank pipeline (nlp = spacy.blank(\"en\"))**, then we need to add every component manually. But if we add **(nlp = spacy.load(\"en_core_web_sm\")) pipeline** then all the components will be added. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc8be4",
   "metadata": {},
   "source": [
    "<img src = \"img4.jpg\" width = \"600px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db6ce22",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "(1) Think stats is a free book to study statistics (https://greenteapress.com/thinkstats2/thinkstats2.pdf)\n",
    "\n",
    "This book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7dbf4f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "# TODO: Write code here\n",
    "# Hint: token has an attribute that can be used to detect a url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed41cc7a",
   "metadata": {},
   "source": [
    "(2) Extract all money transaction from below sentence along with currency. Output should be,\n",
    "\n",
    "two $\n",
    "\n",
    "500 €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e67613d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "\n",
    "# TODO: Write code here\n",
    "# Hint: Use token.i for the index of a token and token.is_currency for currency symbol detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9fbb6de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first find the websites URLs from the text:\n",
    "doce = nlp(text)\n",
    "URLs = []\n",
    "for token in doce:\n",
    "    if token.like_url:\n",
    "        URLs.append(token.text)\n",
    "URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "193270d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "# Next to extract money transaction:\n",
    "doc_e = nlp(transactions)\n",
    "for token in doc_e:\n",
    "    if token.like_num and doc_e[token.i+1].is_currency:\n",
    "        print(token.text, doc_e[token.i+1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeec5a2",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "https://spacy.io/usage/linguistic-features#tokenization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
