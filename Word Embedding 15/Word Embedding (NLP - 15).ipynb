{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1481598",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "**Word embedding** is a technique used in natural language processing (NLP) to represent words as numerical vectors, which can be used as input to machine learning algorithms. Word embedding is a way of mapping words to a high-dimensional space, where each word is represented by a vector of real numbers. These vectors capture the meaning and context of the words, allowing NLP models to better understand and analyze natural language. Word embedding is often used for tasks such as sentiment analysis, text classification, and machine translation. Popular word embedding techniques include Word2Vec, GloVe, and FastText."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89543d74",
   "metadata": {},
   "source": [
    "The certain limitations of **BOW** and **TF-IDF** are:\n",
    "  1. The vector size will be very big, so it may consume too much memory and compute resources. As well as the presentation will be **Sparse.**\n",
    "  2. They both can't captures the meaning of the words properly.\n",
    "    \n",
    "### Word Embedding try to address both of these shortcoming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99546d6",
   "metadata": {},
   "source": [
    "In **Word Embedding:**\n",
    "  1. Similar words or either similar sentences will have similar vectors.\n",
    "  2. The're lower in dimensions. As well as these representations are **dense representations.** It means most of the values are not zeroes.\n",
    "  3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d47433",
   "metadata": {},
   "source": [
    "<img src = \"img.png\" width = \"600px\" height = \"500px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78076bb",
   "metadata": {},
   "source": [
    "* There are various **Word Embedding** techniques:\n",
    "   1. Based on CBOW and Skip gram we can build the following word embeddings:\n",
    "       - Word2Vec\n",
    "       - Glove\n",
    "       - FastText\n",
    "       \n",
    "   2. Transformer based word embedding techniques are:   (These are the latest improvement in NLP.)\n",
    "       - BERT\n",
    "       - GPT\n",
    "    \n",
    "   3. Based on LSTM we have the following word embedding technique:\n",
    "       - ELMO\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6bc087",
   "metadata": {},
   "source": [
    "<img src = \"img1.png\" width = \"600px\" height = \"500px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9fb04",
   "metadata": {},
   "source": [
    "* The main concept of using these techniques is, they convert word or sentence into a vector representation, so it can capture the meaning that word properly. Not only that, we can do mathematics with the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7eee51",
   "metadata": {},
   "source": [
    "* Now, there is some variations for the techniques based on training datasets, for examples **Word2Vec** can be train on Google news dataset or some different corpus. Next is **Glove.** For example **glove-twitter-50** which is include in gensim library, it will understand the twitts better (all the slangs, all the short form...). Next is **glove-wiki-gigaword-100**, there are so many models... but the basic techniques are **Glove**, **Word2Vec**, **FastText** and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e0457",
   "metadata": {},
   "source": [
    "<img src = \"img2.png\" width = \"600px\" height = \"500px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327b4787",
   "metadata": {},
   "source": [
    "* Similarly the transformer based models such as BERT also based on what type of dataset they are trained on, you can get **Bio BERT** or **Fin BERT**. Fin BERT is trained on financial datasets and Bio BERT is trained on Bio medical dataset. We can also do some parameter tuninng in BERT and get things like **ALBERT** or **RoBERTa**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc37942",
   "metadata": {},
   "source": [
    "<img src = \"img3.png\" width = \"600px\" height = \"500px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae1793b",
   "metadata": {},
   "source": [
    "* Just to summarize what these techniques produces is, the whole purpose of these techniques are to convert word into a vector. We know ML don't understand text, they need numbers so these are the ways we can change the texts into numbers. \n",
    "* We can convert single word into a vector or we can convert entire sentence into a vector or we can convert the entire paragraph into a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c382129",
   "metadata": {},
   "source": [
    "<img src = \"img4.png\" width = \"600px\" height = \"500px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c414321",
   "metadata": {},
   "source": [
    "* So to use **Word Embedding** in **Spacy** we need to load the large 'en_core_web_lg' or medium 'en_core_web_md' model. The difference between these two models is, large model has more vectors than medium model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962dcc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# word vectors occupy lot of space. hence en_core_web_sm model do not have them included. \n",
    "# make sure you have run \"python -m spacy download en_core_web_lg\" to install large english model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a63aa81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog Vector:  True OOV:  False\n",
      "cat Vector:  True OOV:  False\n",
      "banana Vector:  True OOV:  False\n",
      "Baktash Vector:  False OOV:  True\n",
      "computer Vector:  True OOV:  False\n"
     ]
    }
   ],
   "source": [
    "# Let's say we want to compare word vectors with different words such 'dog, cat, banana'.\n",
    "# 'token.has_vector' will shows that this word is a vector or not!\n",
    "# OOV will show Out Of Vocabulary.\n",
    "\n",
    "doc = nlp(\"dog cat banana Baktash computer\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \"Vector: \", token.has_vector, \"OOV: \", token.is_oov)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c990a70",
   "metadata": {},
   "source": [
    "* So we see that the general words such as 'dog, cat, banana, computer' have vectors but other random words don't have vectors. The reason is when spacy model was trained it probably hasn't seen the 'Baktash' word. \n",
    "* **Spacy** using **Glove Embedding** (global vectors) and they trained on some popular English datasets. It capture general English knowledge, it doesn't know what the word 'Baktash' means!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558de83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2330e+00,  4.2963e+00, -7.9738e+00, -1.0121e+01,  1.8207e+00,\n",
       "        1.4098e+00, -4.5180e+00, -5.2261e+00, -2.9157e-01,  9.5234e-01,\n",
       "        6.9880e+00,  5.0637e+00, -5.5726e-03,  3.3395e+00,  6.4596e+00,\n",
       "       -6.3742e+00,  3.9045e-02, -3.9855e+00,  1.2085e+00, -1.3186e+00,\n",
       "       -4.8886e+00,  3.7066e+00, -2.8281e+00, -3.5447e+00,  7.6888e-01,\n",
       "        1.5016e+00, -4.3632e+00,  8.6480e+00, -5.9286e+00, -1.3055e+00,\n",
       "        8.3870e-01,  9.0137e-01, -1.7843e+00, -1.0148e+00,  2.7300e+00,\n",
       "       -6.9039e+00,  8.0413e-01,  7.4880e+00,  6.1078e+00, -4.2130e+00,\n",
       "       -1.5384e-01, -5.4995e+00,  1.0896e+01,  3.9278e+00, -1.3601e-01,\n",
       "        7.7732e-02,  3.2218e+00, -5.8777e+00,  6.1359e-01, -2.4287e+00,\n",
       "        6.2820e+00,  1.3461e+01,  4.3236e+00,  2.4266e+00, -2.6512e+00,\n",
       "        1.1577e+00,  5.0848e+00, -1.7058e+00,  3.3824e+00,  3.2850e+00,\n",
       "        1.0969e+00, -8.3711e+00, -1.5554e+00,  2.0296e+00, -2.6796e+00,\n",
       "       -6.9195e+00, -2.3386e+00, -1.9916e+00, -3.0450e+00,  2.4890e+00,\n",
       "        7.3247e+00,  1.3364e+00,  2.3828e-01,  8.4388e-02,  3.1480e+00,\n",
       "       -1.1128e+00, -3.5598e+00, -1.2115e-01, -2.0357e+00, -3.2731e+00,\n",
       "       -7.7205e+00,  4.0948e+00, -2.0732e+00,  2.0833e+00, -2.2803e+00,\n",
       "       -4.9850e+00,  9.7667e+00,  6.1779e+00, -1.0352e+01, -2.2268e+00,\n",
       "        2.5765e+00, -5.7440e+00,  5.5564e+00, -5.2735e+00,  3.0004e+00,\n",
       "       -4.2512e+00, -1.5682e+00,  2.2698e+00,  1.0491e+00, -9.0486e+00,\n",
       "        4.2936e+00,  1.8709e+00,  5.1985e+00, -1.3153e+00,  6.5224e+00,\n",
       "        4.0113e-01, -1.2583e+01,  3.6534e+00, -2.0961e+00,  1.0022e+00,\n",
       "       -1.7873e+00, -4.2555e+00,  7.7471e+00,  1.0173e+00,  3.1626e+00,\n",
       "        2.3558e+00,  3.3589e-01, -4.4178e+00,  5.0584e+00, -2.4118e+00,\n",
       "       -2.7445e+00,  3.4170e+00, -1.1574e+01, -2.6568e+00, -3.6933e+00,\n",
       "       -2.0398e+00,  5.0976e+00,  6.5249e+00,  3.3573e+00,  9.5334e-01,\n",
       "       -9.4430e-01, -9.4395e+00,  2.7867e+00, -1.7549e+00,  1.7287e+00,\n",
       "        3.4942e+00, -1.6883e+00, -3.5771e+00, -1.9013e+00,  2.2239e+00,\n",
       "       -5.4335e+00, -6.5724e+00, -6.7228e-01, -1.9748e+00, -3.1080e+00,\n",
       "       -1.8570e+00,  9.9496e-01,  8.9135e-01, -4.4254e+00,  3.3125e-01,\n",
       "        5.8815e+00,  1.9384e+00,  5.7294e-01, -2.8830e+00,  3.8087e+00,\n",
       "       -1.3095e+00,  5.9208e+00,  3.3620e+00,  3.3571e+00, -3.8807e-01,\n",
       "        9.0022e-01, -5.5742e+00, -4.2939e+00,  1.4992e+00, -4.7080e+00,\n",
       "       -2.9402e+00, -1.2259e+00,  3.0980e-01,  1.8858e+00, -1.9867e+00,\n",
       "       -2.3554e-01, -5.4535e-01, -2.1387e-01,  2.4797e+00,  5.9710e+00,\n",
       "       -7.1249e+00,  1.6257e+00, -1.5241e+00,  7.5974e-01,  1.4312e+00,\n",
       "        2.3641e+00, -3.5566e+00,  9.2066e-01,  4.4934e-01, -1.3233e+00,\n",
       "        3.1733e+00, -4.7059e+00, -1.2090e+01, -3.9241e-01, -6.8457e-01,\n",
       "       -3.6789e+00,  6.6279e+00, -2.9937e+00, -3.8361e+00,  1.3868e+00,\n",
       "       -4.9002e+00, -2.4299e+00,  6.4312e+00,  2.5056e+00, -4.5080e+00,\n",
       "       -5.1278e+00, -1.5585e+00, -3.0226e+00, -8.6811e-01, -1.1538e+00,\n",
       "       -1.0022e+00, -9.1651e-01, -4.7810e-01, -1.6084e+00, -2.7307e+00,\n",
       "        3.7080e+00,  7.7423e-01, -1.1085e+00, -6.8755e-01, -8.2901e+00,\n",
       "        3.2405e+00, -1.6108e-01, -6.2837e-01, -5.5960e+00, -4.4865e+00,\n",
       "        4.0115e-01, -3.7063e+00, -2.1704e+00,  4.0789e+00, -1.7973e+00,\n",
       "        8.9538e+00,  8.9421e-01, -4.8128e+00,  4.5367e+00, -3.2579e-01,\n",
       "       -5.2344e+00, -3.9766e+00, -2.1979e+00,  3.5699e+00,  1.4982e+00,\n",
       "        6.0972e+00, -1.9704e+00,  4.6522e+00, -3.7734e-01,  3.9101e-02,\n",
       "        2.5361e+00, -1.8096e+00,  8.7035e+00, -8.6372e+00, -3.5257e+00,\n",
       "        3.1034e+00,  3.2635e+00,  4.5437e+00, -5.7290e+00, -2.9141e-01,\n",
       "       -2.0011e+00,  8.5328e+00, -4.5064e+00, -4.8276e+00, -1.1786e+01,\n",
       "        3.5607e-01, -5.7115e+00,  6.3122e+00, -3.6650e+00,  3.3597e-01,\n",
       "        2.5017e+00, -3.5025e+00, -3.7891e+00, -3.1343e+00, -1.4429e+00,\n",
       "       -6.9119e+00, -2.6114e+00, -5.9757e-01,  3.7847e-01,  6.3187e+00,\n",
       "        2.8965e+00, -2.5397e+00,  1.8022e+00,  3.5486e+00,  4.4721e+00,\n",
       "       -4.8481e+00, -3.6252e+00,  4.0969e+00, -2.0081e+00, -2.0122e-01,\n",
       "        2.5244e+00, -6.8817e-01,  6.7184e-01, -7.0466e+00,  1.6641e+00,\n",
       "       -2.2308e+00, -3.8960e+00,  6.1320e+00, -8.0335e+00, -1.7130e+00,\n",
       "        2.5688e+00, -5.2547e+00,  6.9845e+00,  2.7835e-01, -6.4554e+00,\n",
       "       -2.1327e+00, -5.6515e+00,  1.1174e+01, -8.0568e+00,  5.7985e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to print a prticular vector, you can specify it by index.\n",
    "doc[0].vector   # will print vector for word 'dog'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa2414f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To know about the size or dimension of the vector:\n",
    "doc[0].vector.shape # will output that it's 300 dimension vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f7e555",
   "metadata": {},
   "source": [
    "* So **spacy large model** come with in build vectors, so in the moment we created the document 'doc = nlp (\"dog ...\")', we were able to get the vectors for the defined words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "677f20e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's compare these vectors and see what kind of similary we get between these words!\n",
    "# So we'll create another document called 'bread'.\n",
    "base_token = nlp(\"bread\")\n",
    "base_token.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8a57067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bread <-> bread: 1.0\n",
      "sandwich <-> bread: 0.6341067010130894\n",
      "burger <-> bread: 0.47520687769584247\n",
      "car <-> bread: 0.06451532596945217\n",
      "tiger <-> bread: 0.04764611272488976\n",
      "human <-> bread: 0.2151154210812192\n",
      "wheat <-> bread: 0.615036141030184\n"
     ]
    }
   ],
   "source": [
    "# Now we want to compare the 'bread' vectors with different words such as 'sandwich ....'.\n",
    "# So we know that 'bread' and 'sandwich & burger' words are a kind similar, so we predict that their similarity will be \n",
    "# hihger, and with other words the similarity will be lower.\n",
    "# So the way they produce the similarity is, it keep the context of the words, if two words are repeated with each other, \n",
    "# then they will have more similarity. Here the similarity is based on the context, 'loss' and 'profit' word will have more\n",
    "# similarity, now some one will says that why they are similar? they're opposite of each other! The answer is, this similary\n",
    "# is based on context of the words.\n",
    "doc = nlp(\"bread sandwich burger car tiger human wheat\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text} <-> {base_token.text}:\", token.similarity(base_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "458ae4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll create a function for words similarity.\n",
    "\n",
    "def print_similarity(base_word, words_to_compare):\n",
    "    base_token = nlp(base_word)\n",
    "    doc = nlp(words_to_compare)\n",
    "    for token in doc:\n",
    "        print(f\"{token.text} <-> {base_token.text}: \", token.similarity(base_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a5b51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple <-> iphone:  0.4387907748060368\n",
      "samsung <-> iphone:  0.6708590303423401\n",
      "iphone <-> iphone:  1.0\n",
      "dog <-> iphone:  0.08211864228011527\n",
      "kitten <-> iphone:  0.10222317834969896\n"
     ]
    }
   ],
   "source": [
    "# We'll compare 'iphone' with 'apple', 'samsun' ... words.\n",
    "print_similarity(\"iphone\", \"apple samsung iphone dog kitten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "328bb762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have some mathematics with these feature vectors:\n",
    "king = nlp.vocab[\"king\"].vector\n",
    "man = nlp.vocab[\"man\"].vector\n",
    "woman = nlp.vocab[\"woman\"].vector\n",
    "queen = nlp.vocab[\"queen\"].vector\n",
    "\n",
    "result = king - man + woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a8053b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6178014]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now if you compared the vector of 'result' with the vector of 'queen', it would be a kind similar.\n",
    "# For similarity we'll use cosine similarity.\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([result], [queen])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
