{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4710eb0b",
   "metadata": {},
   "source": [
    "## News Classification using Gensim Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d2e1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So first we load Google News Word2vec model from gensim library:\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d78b916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_adapt_by_suffix',\n",
       " '_load_specials',\n",
       " '_log_evaluate_word_analogies',\n",
       " '_save_specials',\n",
       " '_smart_save',\n",
       " '_upconvert_old_d2vkv',\n",
       " '_upconvert_old_vocab',\n",
       " 'add_lifecycle_event',\n",
       " 'add_vector',\n",
       " 'add_vectors',\n",
       " 'allocate_vecattrs',\n",
       " 'closer_than',\n",
       " 'cosine_similarities',\n",
       " 'distance',\n",
       " 'distances',\n",
       " 'doesnt_match',\n",
       " 'evaluate_word_analogies',\n",
       " 'evaluate_word_pairs',\n",
       " 'expandos',\n",
       " 'fill_norms',\n",
       " 'get_index',\n",
       " 'get_normed_vectors',\n",
       " 'get_vecattr',\n",
       " 'get_vector',\n",
       " 'has_index_for',\n",
       " 'index2entity',\n",
       " 'index2word',\n",
       " 'index_to_key',\n",
       " 'init_sims',\n",
       " 'intersect_word2vec_format',\n",
       " 'key_to_index',\n",
       " 'lifecycle_events',\n",
       " 'load',\n",
       " 'load_word2vec_format',\n",
       " 'log_accuracy',\n",
       " 'log_evaluate_word_pairs',\n",
       " 'mapfile_path',\n",
       " 'most_similar',\n",
       " 'most_similar_cosmul',\n",
       " 'most_similar_to_given',\n",
       " 'n_similarity',\n",
       " 'next_index',\n",
       " 'norms',\n",
       " 'rank',\n",
       " 'rank_by_centrality',\n",
       " 'relative_cosine_similarity',\n",
       " 'resize_vectors',\n",
       " 'save',\n",
       " 'save_word2vec_format',\n",
       " 'set_vecattr',\n",
       " 'similar_by_key',\n",
       " 'similar_by_vector',\n",
       " 'similar_by_word',\n",
       " 'similarity',\n",
       " 'similarity_unseen_docs',\n",
       " 'sort_by_descending_frequency',\n",
       " 'unit_normalize_all',\n",
       " 'vector_size',\n",
       " 'vectors',\n",
       " 'vectors_for_all',\n",
       " 'vectors_norm',\n",
       " 'vocab',\n",
       " 'wmdistance',\n",
       " 'word_vec',\n",
       " 'words_closer_than']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6979e4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72915095"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again to simple check similarity between two words:\n",
    "wv.similarity(w1=\"great\", w2=\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af309b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lenght for each vector is 300:\n",
    "wv_great = wv[\"great\"]\n",
    "wv_good = wv[\"good\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa79e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_great.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b7830e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.17773438e-02,  2.08007812e-01, -2.84423828e-02,  1.78710938e-01,\n",
       "        1.32812500e-01, -9.96093750e-02,  9.61914062e-02, -1.16699219e-01,\n",
       "       -8.54492188e-03,  1.48437500e-01, -3.34472656e-02, -1.85546875e-01,\n",
       "        4.10156250e-02, -8.98437500e-02,  2.17285156e-02,  6.93359375e-02,\n",
       "        1.80664062e-01,  2.22656250e-01, -1.00585938e-01, -6.93359375e-02,\n",
       "        1.04427338e-04,  1.60156250e-01,  4.07714844e-02,  7.37304688e-02,\n",
       "        1.53320312e-01,  6.78710938e-02, -1.03027344e-01,  4.17480469e-02,\n",
       "        4.27246094e-02, -1.10351562e-01, -6.68945312e-02,  4.19921875e-02,\n",
       "        2.50000000e-01,  2.12890625e-01,  1.59179688e-01,  1.44653320e-02,\n",
       "       -4.88281250e-02,  1.39770508e-02,  3.55529785e-03,  2.09960938e-01,\n",
       "        1.52343750e-01, -7.32421875e-02,  2.16796875e-01, -5.76171875e-02,\n",
       "       -2.84423828e-02, -3.60107422e-03,  1.52343750e-01, -2.63671875e-02,\n",
       "        2.13623047e-02, -1.51367188e-01,  1.04003906e-01,  3.18359375e-01,\n",
       "       -1.85546875e-01,  3.68652344e-02, -1.10839844e-01, -3.17382812e-02,\n",
       "       -1.01562500e-01, -1.21093750e-01,  3.22265625e-01, -7.32421875e-02,\n",
       "       -1.52343750e-01,  2.67578125e-01, -1.50390625e-01, -1.23046875e-01,\n",
       "        1.07910156e-01,  6.68945312e-02, -2.13623047e-02, -1.00585938e-01,\n",
       "       -2.05078125e-01,  1.17675781e-01,  6.15234375e-02,  6.78710938e-02,\n",
       "        1.06933594e-01, -7.71484375e-02, -1.52343750e-01, -4.24194336e-03,\n",
       "       -1.45507812e-01,  2.53906250e-01,  4.80957031e-02,  9.71679688e-02,\n",
       "       -8.36181641e-03,  1.12792969e-01,  5.34667969e-02,  1.79443359e-02,\n",
       "       -5.63964844e-02, -3.30078125e-01, -9.76562500e-02,  1.42578125e-01,\n",
       "       -1.37695312e-01,  2.20947266e-02,  1.00097656e-01, -5.71289062e-02,\n",
       "       -1.56250000e-01, -6.37817383e-03, -9.37500000e-02, -4.68750000e-02,\n",
       "        8.59375000e-02,  3.06640625e-01, -1.11328125e-01, -1.94335938e-01,\n",
       "       -2.08007812e-01,  8.10546875e-02, -4.19921875e-02, -8.30078125e-02,\n",
       "       -1.04003906e-01,  2.92968750e-01,  2.39257812e-02, -3.85742188e-02,\n",
       "        3.56445312e-02, -1.04980469e-01, -6.54296875e-02,  2.79296875e-01,\n",
       "       -1.16210938e-01, -1.45874023e-02,  3.84765625e-01, -7.81250000e-02,\n",
       "       -2.92968750e-02, -1.35742188e-01, -5.39550781e-02, -5.49316406e-02,\n",
       "       -8.10546875e-02, -2.88085938e-02,  8.34960938e-02,  2.73437500e-01,\n",
       "       -6.20117188e-02, -4.78515625e-02, -1.09252930e-02, -1.13769531e-01,\n",
       "       -1.09863281e-01,  2.02148438e-01, -1.28906250e-01, -6.68945312e-02,\n",
       "       -2.67578125e-01,  9.61914062e-02,  1.04003906e-01, -1.69921875e-01,\n",
       "        5.56640625e-02,  1.54296875e-01,  8.05664062e-02,  2.19726562e-01,\n",
       "       -2.27539062e-01,  1.10351562e-01, -8.11767578e-03, -5.63964844e-02,\n",
       "       -9.03320312e-02, -7.76367188e-02, -3.61328125e-02,  3.61328125e-02,\n",
       "        1.58203125e-01, -1.56250000e-01,  2.26562500e-01,  2.85156250e-01,\n",
       "       -5.51757812e-02,  3.53515625e-01, -1.20605469e-01,  1.05957031e-01,\n",
       "        3.11279297e-02, -1.91406250e-01, -2.31445312e-01, -1.11816406e-01,\n",
       "        2.38037109e-03,  7.51953125e-02, -1.28784180e-02,  1.00585938e-01,\n",
       "        4.45312500e-01, -2.77343750e-01,  6.68945312e-02, -8.10546875e-02,\n",
       "        6.39648438e-02,  1.85546875e-02, -1.11328125e-01,  9.76562500e-02,\n",
       "        2.06054688e-01, -1.30859375e-01,  2.39257812e-02,  1.10839844e-01,\n",
       "        8.05664062e-02, -1.52343750e-01,  4.85229492e-03,  1.84326172e-02,\n",
       "       -9.17968750e-02, -2.41210938e-01,  8.39843750e-02, -1.00585938e-01,\n",
       "       -1.54296875e-01,  2.75878906e-02, -1.64062500e-01, -1.01562500e-01,\n",
       "       -6.07299805e-03,  1.33514404e-03, -2.53906250e-01,  3.14453125e-01,\n",
       "        1.31835938e-01, -1.31835938e-01,  2.17285156e-02, -1.56250000e-01,\n",
       "       -1.46484375e-01, -5.12695312e-02, -1.20605469e-01, -2.15820312e-01,\n",
       "        3.10058594e-02,  1.30859375e-01,  9.71679688e-02,  5.67626953e-03,\n",
       "        2.20947266e-02,  1.26953125e-01, -1.24511719e-02,  6.15234375e-02,\n",
       "       -2.23388672e-02,  2.50000000e-01, -7.17773438e-02,  1.58203125e-01,\n",
       "       -7.27539062e-02,  1.97753906e-02,  8.85009766e-03, -9.08203125e-02,\n",
       "        3.63281250e-01, -9.03320312e-02,  2.41699219e-02, -1.39770508e-02,\n",
       "       -5.10253906e-02,  2.40478516e-02,  5.88989258e-03, -1.02050781e-01,\n",
       "       -8.85009766e-03,  3.05175781e-02, -7.81250000e-02, -1.27929688e-01,\n",
       "        3.85742188e-02,  2.86865234e-02, -2.28515625e-01, -1.25122070e-02,\n",
       "        1.54296875e-01,  9.13085938e-02,  1.05468750e-01, -6.44531250e-02,\n",
       "       -1.28906250e-01, -1.02050781e-01, -2.16064453e-02, -3.29589844e-02,\n",
       "        7.47070312e-02,  3.78417969e-02,  7.42187500e-02, -1.23901367e-02,\n",
       "       -4.68750000e-02,  4.88281250e-03,  1.03515625e-01, -8.69140625e-02,\n",
       "       -2.26562500e-01, -2.53906250e-01,  3.58886719e-02,  4.45312500e-01,\n",
       "        5.56640625e-02,  1.59179688e-01,  2.71484375e-01, -1.08398438e-01,\n",
       "        6.25000000e-02, -5.59082031e-02, -2.50000000e-01, -1.55273438e-01,\n",
       "       -6.83593750e-02, -1.39648438e-01, -1.59179688e-01, -1.79443359e-02,\n",
       "        2.12402344e-02,  7.37304688e-02,  1.30859375e-01, -8.05664062e-02,\n",
       "        2.99072266e-02,  1.55639648e-02, -1.66015625e-01,  1.50390625e-01,\n",
       "       -6.77490234e-03,  1.01318359e-02,  1.14746094e-01, -1.48437500e-01,\n",
       "       -4.58984375e-02, -1.39648438e-01, -1.73828125e-01, -4.27246094e-02,\n",
       "       -5.81054688e-02,  5.22460938e-02, -1.11328125e-01,  8.44726562e-02,\n",
       "       -2.55126953e-02,  1.40625000e-01, -1.81640625e-01,  1.72119141e-02,\n",
       "       -1.37695312e-01, -1.47705078e-02, -1.14746094e-02,  6.44531250e-02,\n",
       "       -2.89062500e-01, -4.80957031e-02, -1.99218750e-01, -7.12890625e-02,\n",
       "        6.44531250e-02, -1.67968750e-01, -2.08740234e-02, -1.42578125e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the vector:\n",
    "wv_great"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba1008a",
   "metadata": {},
   "source": [
    "### Fake vs Real News Classification Using This Word2Vec Embeddings\n",
    "* Fake news refers to misinformation or disinformation in the country which is spread through word of mouth and more recently through digital communication such as What's app messages, social media posts, etc.\n",
    "\n",
    "* Fake news spreads faster than real news and creates problems and fear among groups and in society.\n",
    "\n",
    "* We are going to address these problems using classical NLP techniques and going to classify whether a given message/ text is **Real or Fake Message**.\n",
    "\n",
    "* We will use **glove embeddings** from spacy which is trained on massive wikipedia dataset to pre-process and text vectorization and apply different classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28efa59d",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Credits: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
    "\n",
    "* This data consists of two columns. - Text - label\n",
    "* Text is the statements or messages regarding a particular event/situation.\n",
    "* label feature tells whether the given text is Fake or Real.\n",
    "* As there are only 2 classes, this problem comes under the Binary Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed4af9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5445</th>\n",
       "      <td>Factbox: Trump on Twitter (Sept 25) - NASCAR, ...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>Trump slaps travel restrictions on N.Korea, Ve...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8581</th>\n",
       "      <td>Republican Senators Corker, Toomey reach deal ...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>Trump Just Started Following ‘Emergency Kitte...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8445</th>\n",
       "      <td>Former Trump aide nomination to be Singapore e...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text label\n",
       "5445  Factbox: Trump on Twitter (Sept 25) - NASCAR, ...  Real\n",
       "1775  Trump slaps travel restrictions on N.Korea, Ve...  Real\n",
       "8581  Republican Senators Corker, Toomey reach deal ...  Real\n",
       "2155   Trump Just Started Following ‘Emergency Kitte...  Fake\n",
       "8445  Former Trump aide nomination to be Singapore e...  Real"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So let's import pandas and read the dataset:\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"fake_and_real_news.csv\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc8ae2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9900, 2)\n"
     ]
    }
   ],
   "source": [
    "# To see the shape of the dataset:\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b6201c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fake    5000\n",
       "Real    4900\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the imbalance of dataset:\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fee4f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top Trump Surrogate BRUTALLY Stabs Him In The...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. conservative leader optimistic of common ...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump proposes U.S. tax overhaul, stirs concer...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Court Forces Ohio To Allow Millions Of Illega...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democrats say Trump agrees to work on immigrat...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text label  label_num\n",
       "0   Top Trump Surrogate BRUTALLY Stabs Him In The...  Fake          0\n",
       "1  U.S. conservative leader optimistic of common ...  Real          1\n",
       "2  Trump proposes U.S. tax overhaul, stirs concer...  Real          1\n",
       "3   Court Forces Ohio To Allow Millions Of Illega...  Fake          0\n",
       "4  Democrats say Trump agrees to work on immigrat...  Real          1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So as we see both classes samples are almost similar, so no need for further processing.\n",
    "# Next we change the label from text to numbers by creating a new column:\n",
    "\n",
    "df['label_num'] = df['label'].map({'Fake' : 0, 'Real': 1})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7d11d",
   "metadata": {},
   "source": [
    "**Now we will convert the text into a vector using gensim's word2vec embeddings.**\n",
    "\n",
    "**We will do this in three steps,**\n",
    "   1. Preprocess the text to remove stop words, punctuations and get lemma for each word.\n",
    "   2. Get word vectors for each of the words in a pre-processed sentece.\n",
    "   3. Take a mean of all word vectors to derive the numeric representation of the entire news article.\n",
    "First let's explore get_mean_vector api of gensim to see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc6de584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's write the function that can do preprocessing and vectorization both:\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\") \n",
    "\n",
    "def preprocess_and_vectorize(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7dce1309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worry', 'understand']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check the function, if it works or not:\n",
    "preprocess_and_vectorize(\"Don't worry if you don't understand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d81723c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport spacy\\nnlp = spacy.load(\"en_core_web_lg\") \\n\\ndef preprocess_and_vectorize(text):\\n    doc = nlp(text)\\n    filtered_tokens = []\\n    for token in doc:\\n        if token.is_stop or token.is_punct:\\n            continue\\n        filtered_tokens.append(token.lemma_)\\n\\n    return wv.get_mean_vector(filtered_tokens)\\n    '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next we use 'get_mean_vector' to return the mean of all filtered tokens. The reason why we do this is, as we saw that the \n",
    "# filtered tokens 'filtered_tokens' is a list of words in a sentence and we're interested in the embedding of the entire \n",
    "# sentence.\n",
    "'''\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\") \n",
    "\n",
    "def preprocess_and_vectorize(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "\n",
    "    return wv.get_mean_vector(filtered_tokens)\n",
    "    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d6caac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = preprocess_and_vectorize(\"Don't worry if you don't understand\")\n",
    "# v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbeb17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we want to do vectorization. We create a new column 'vector' which will have a vectors for the 'Text' column \n",
    "df['vector'] = df['Text'].apply(lambda text: preprocess_and_vectorize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56dda682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top Trump Surrogate BRUTALLY Stabs Him In The...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "      <td>[ , Trump, Surrogate, BRUTALLY, Stabs, Patheti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. conservative leader optimistic of common ...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "      <td>[U.S., conservative, leader, optimistic, commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump proposes U.S. tax overhaul, stirs concer...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "      <td>[Trump, propose, U.S., tax, overhaul, stir, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Court Forces Ohio To Allow Millions Of Illega...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>0</td>\n",
       "      <td>[ , Court, Forces, Ohio, allow, million, illeg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democrats say Trump agrees to work on immigrat...</td>\n",
       "      <td>Real</td>\n",
       "      <td>1</td>\n",
       "      <td>[Democrats, Trump, agree, work, immigration, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text label  label_num  \\\n",
       "0   Top Trump Surrogate BRUTALLY Stabs Him In The...  Fake          0   \n",
       "1  U.S. conservative leader optimistic of common ...  Real          1   \n",
       "2  Trump proposes U.S. tax overhaul, stirs concer...  Real          1   \n",
       "3   Court Forces Ohio To Allow Millions Of Illega...  Fake          0   \n",
       "4  Democrats say Trump agrees to work on immigrat...  Real          1   \n",
       "\n",
       "                                              vector  \n",
       "0  [ , Trump, Surrogate, BRUTALLY, Stabs, Patheti...  \n",
       "1  [U.S., conservative, leader, optimistic, commo...  \n",
       "2  [Trump, propose, U.S., tax, overhaul, stir, co...  \n",
       "3  [ , Court, Forces, Ohio, allow, million, illeg...  \n",
       "4  [Democrats, Trump, agree, work, immigration, b...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to see the first five rows:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So next we split the dataset into train and test samples:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.vector.values, \n",
    "    df.label_num, \n",
    "    test_size=0.2,\n",
    "    random_state=2022,\n",
    "    stratify=df.label_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we reshape the X_train and X_test so as to fit for models:\n",
    "print(\"Shape of X_train before reshaping: \", X_train.shape)\n",
    "print(\"Shape of X_test before reshaping: \", X_test.shape)\n",
    "\n",
    "\n",
    "X_train_2d = np.stack(X_train)\n",
    "X_test_2d =  np.stack(X_test)\n",
    "\n",
    "print(\"Shape of X_train after reshaping: \", X_train_2d.shape)\n",
    "print(\"Shape of X_test after reshaping: \", X_test_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we train the model.\n",
    "# Here we tried to train Gradient Boosting Classifier as it probably will get best result comparing with RF, NB and DT.\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train_2d, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_2d)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf84a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some prediction:\n",
    "test_news = [\n",
    "    \"Michigan governor denies misleading U.S. House on Flint water (Reuters) - Michigan Governor Rick Snyder denied Thursday that he had misled a U.S. House of Representatives committee last year over testimony on Flintâ€™s water crisis after lawmakers asked if his testimony had been contradicted by a witness in a court hearing. The House Oversight and Government Reform Committee wrote Snyder earlier Thursday asking him about published reports that one of his aides, Harvey Hollins, testified in a court hearing last week in Michigan that he had notified Snyder of an outbreak of Legionnairesâ€™ disease linked to the Flint water crisis in December 2015, rather than 2016 as Snyder had testified. â€œMy testimony was truthful and I stand by it,â€ Snyder told the committee in a letter, adding that his office has provided tens of thousands of pages of records to the committee and would continue to cooperate fully.  Last week, prosecutors in Michigan said Dr. Eden Wells, the stateâ€™s chief medical executive who already faced lesser charges, would become the sixth current or former official to face involuntary manslaughter charges in connection with the crisis. The charges stem from more than 80 cases of Legionnairesâ€™ disease and at least 12 deaths that were believed to be linked to the water in Flint after the city switched its source from Lake Huron to the Flint River in April 2014. Wells was among six current and former Michigan and Flint officials charged in June. The other five, including Michigan Health and Human Services Director Nick Lyon, were charged at the time with involuntary manslaughter\",\n",
    "    \" WATCH: Fox News Host Loses Her Sh*t, Says Investigating Russia For Hacking Our Election Is Unpatriotic This woman is insane.In an incredibly disrespectful rant against President Obama and anyone else who supports investigating Russian interference in our election, Fox News host Jeanine Pirro said that anybody who is against Donald Trump is anti-American. Look, it s time to take sides,  she began.\",\n",
    "    \" Sarah Palin Celebrates After White Man Who Pulled Gun On Black Protesters Goes Unpunished (VIDEO) Sarah Palin, one of the nigh-innumerable  deplorables  in Donald Trump s  basket,  almost outdid herself in terms of horribleness on Friday.\"\n",
    "]\n",
    "\n",
    "test_news_vectors = [preprocess_and_vectorize(n) for n in test_news]\n",
    "clf.predict(test_news_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Truth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
