{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc5d7cc5",
   "metadata": {},
   "source": [
    "## Spacy Language Processing Pipeline \n",
    "**Pipeline** is basically a bunch of components. When you do 'nlp = spacy.blank(\"en\"), this will create a blank pipeline. So you take text as input, then you do tokenizer and then there are couple of components to be added to this blank pipeline as shown in image bellow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b10469f",
   "metadata": {},
   "source": [
    "<img src = \"img.jpg\" width = \"600px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47106993",
   "metadata": {},
   "source": [
    "So the question is what are those components? Well those components will be 'tagger', 'parser', named entity recognization (ner) ... as shown in image bellow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdad894",
   "metadata": {},
   "source": [
    "<img src = \"img1.jpg\" width = \"600px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a103e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import spacy library.\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d1ddb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain\n",
      "america\n",
      "ate\n",
      "100\n",
      "$\n",
      "of\n",
      "samosa\n",
      ".\n",
      "Then\n",
      "he\n",
      "said\n",
      "I\n",
      "can\n",
      "do\n",
      "this\n",
      "all\n",
      "day\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# So let's first create a blank spacy pipeline and then add some text to the language model:\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab518aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So as we see our tokenizer tokens the text, because tokenizer is created by default while creating blank spacy pipeline.\n",
    "# If we see the components of current created blank pipeline, it will be blank, becaus we created blank and still we \n",
    "# didn't add any component:\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75d4305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we'll use some pre-trained pipelines with different components.\n",
    "# So if we go to 'Spacy' documentation, for each language we'll find some pre-trained pipelines. For English we installed \n",
    "# previously using 'python -m spacy download en' command.\n",
    "# So once we have downloaded the pre-trained pipeline, then instead of blank pipeline we will create a pipeline which has all\n",
    "# the components.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bfbf4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1abd0056fa0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1abcfaf6b80>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1abd03dac80>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1abd03604c0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1abd0367440>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1abd03da510>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or we can do:\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660d1d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain  |  PROPN  |  Captain\n",
      "america  |  PROPN  |  america\n",
      "ate  |  VERB  |  eat\n",
      "100  |  NUM  |  100\n",
      "$  |  NUM  |  $\n",
      "of  |  ADP  |  of\n",
      "samosa  |  PROPN  |  samosa\n",
      ".  |  PUNCT  |  .\n",
      "Then  |  ADV  |  then\n",
      "he  |  PRON  |  he\n",
      "said  |  VERB  |  say\n",
      "I  |  PRON  |  I\n",
      "can  |  AUX  |  can\n",
      "do  |  VERB  |  do\n",
      "this  |  PRON  |  this\n",
      "all  |  DET  |  all\n",
      "day  |  NOUN  |  day\n",
      ".  |  PUNCT  |  .\n"
     ]
    }
   ],
   "source": [
    "# So now using this components we can do much things:\n",
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \", token.lemma_)  # 'pos' is part of speech. 'lemma_' shows the base word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f4322a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "# So because of 'tagger' component we did 'pos' and through 'lemmatizer' component we did 'lemm_'.\n",
    "# 'ner' will do name entity recognization.\n",
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,\" | \", ent.label_,\" | \",spacy.explain(ent.label_))# will show us the name of words. ORG is organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52a19b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we understand when we loadded a train pipeline with all the components, in reality we get some in build features.\n",
    "# So the components is nothing but the language process pipeline as shown bellow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ca782",
   "metadata": {},
   "source": [
    "<img src = \"img2.jpg\" width = \"600px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eae6b7",
   "metadata": {},
   "source": [
    "* So for some of the language the pre-trained pipelines are exist in spacy documents but for other languages it's not defined. The other languages just jave the basic tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d51df5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For better visualization we can use 'displacy' model:\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "562511f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |    |  \n",
      "Inc  |    |  \n",
      "is  |    |  \n",
      "going  |    |  \n",
      "to  |    |  \n",
      "acquire  |    |  \n",
      "twitter  |    |  \n",
      "for  |    |  \n",
      "$  |    |  \n",
      "45  |    |  \n",
      "billion  |    |  \n"
     ]
    }
   ],
   "source": [
    "# When we're using plain pipeline we won't get 'pos' or 'lemma_', becaus it's plain and didn't give any thing:\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c734c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now if we want to have a specific components in the pipeline, we don't want to have all the components. So for that first \n",
    "# We load the English pipeline, then we create a blank pipeline and in that blank pipeline we add 'enr' and define the source\n",
    "# from which we added it. Which means from English pipeline add 'ner' component to the blank pipeline.\n",
    "\n",
    "source_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"ner\", source=source_nlp)\n",
    "nlp.pipe_names   # It will shows us the added component which is 'ner'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93ba9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can customize our blank pipeline and add custom components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
