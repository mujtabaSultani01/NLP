{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6651f34f",
   "metadata": {},
   "source": [
    "## Stemming & Lemmatization\n",
    "In NLP tasks, **stemming** refers to the process of reducing a word to its base or root form, known as the stem. This is done by removing any suffixes or prefixes that may be attached to the word. The purpose of stemming is to reduce the number of unique words in a text corpus, which can help to simplify analysis and improve efficiency. Stemming is often used in text mining, search engines, and information retrieval systems. However, stemming can sometimes lead to errors or loss of meaning, as some words may have multiple stems or may not be related in meaning despite having the same stem.\n",
    "\n",
    "In NLP tasks, **lemmatization** refers to the process of reducing a word to its base or dictionary form, known as the lemma. This is done by considering the context and morphological analysis of the word. Unlike stemming, which simply removes suffixes and prefixes, lemmatization takes into account the part of speech of the word and its inflectional forms. The purpose of lemmatization is to reduce the number of unique words in a text corpus while preserving the meaning of the words. Lemmatization is often used in natural language processing tasks such as text classification, sentiment analysis, and machine translation. However, lemmatization can be computationally expensive compared to stemming.\n",
    "\n",
    "These two are the essential parts that we need to perform in preprocessing stage while building an NLP application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a725c30",
   "metadata": {},
   "source": [
    "In some cases when we want to make a base form of a word we just use a fix rules like removing 'ing' and 'able' suffixes from the words, so this process is called **Stemming.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be865ed",
   "metadata": {},
   "source": [
    "But sometimes in some special cases the fix rules doens't word ok, we need some further processing, we have to use knowledge of the language to find the base form of the word. The base form of the word is called 'lemma'. And the process is called **Lemmatization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc956c17",
   "metadata": {},
   "source": [
    "<img src = \"img.jpg\" width = \"600px\" heigh = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e211ab0b",
   "metadata": {},
   "source": [
    "**Stemming** always doesn't give correct base form of the word, that's why we need **Lemmatization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5274ad",
   "metadata": {},
   "source": [
    "<img src = \"img1.jpg\" width = \"600px\" heigh = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a517b86d",
   "metadata": {},
   "source": [
    "* So for **Stimming** we'll use **NLTK**, because **Spacy** doesn't have support for stemming, it has just **Lemmatization** support. But **NLTK** supports both **Stemming** and **Lemmatization.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cdbaf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's first import NLTK and Spacy libraries:\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f9d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we import 'PorterStemmer'class from NLTK and we create the object of this class.\n",
    "# There is other Stemmer as well called 'SnowballStemmer' which you can use it.\n",
    "from nltk import PorterStemmer\n",
    "# from nltk import SnowballStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26cdd9e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n"
     ]
    }
   ],
   "source": [
    "# So here we have some words, we want to print the base words for these words using stemmer.\n",
    "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word, \"|\", stemmer.stem(word))   # It will apply fix set of rules. It will make some mistakes because it doesn't \n",
    "                                           # has language knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e00692bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "eats  |  eat\n",
      "eat  |  eat\n",
      "ate  |  eat\n",
      "adjustable  |  adjustable\n",
      "rafting  |  raft\n",
      "ability  |  ability\n",
      "meeting  |  meeting\n",
      "better  |  well\n"
     ]
    }
   ],
   "source": [
    "# Now let's use 'Lemmatization' in 'Spacy'.\n",
    "# First we create English language pre-trained pipeline. Then we defined couple of texts. And then we apply lemmatizaion.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Mando talked for 3 hours although talking isn't his thing\")\n",
    "doc = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_) # Based on the trained model it will do lemmatization. Lemmatization will also have\n",
    "                                      # some errors, it works based on pre-trained English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3cf56c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat  |  9837207709914848172\n",
      "eats  |  eat  |  9837207709914848172\n",
      "eat  |  eat  |  9837207709914848172\n",
      "ate  |  eat  |  9837207709914848172\n",
      "adjustable  |  adjustable  |  6033511944150694480\n",
      "rafting  |  raft  |  7154368781129989833\n",
      "ability  |  ability  |  11565809527369121409\n",
      "meeting  |  meeting  |  14798207169164081740\n",
      "better  |  well  |  4525988469032889948\n"
     ]
    }
   ],
   "source": [
    "# 'lemma' will print unique identifier for each word:\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Mando talked for 3 hours although talking isn't his thing\")\n",
    "doc = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_, \" | \", token.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca38c1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming  |  stemming\n",
      "is  |  be\n",
      "the  |  the\n",
      "process  |  process\n",
      "of  |  of\n",
      "reducing  |  reduce\n",
      "a  |  a\n",
      "word  |  word\n",
      "to  |  to\n",
      "its  |  its\n",
      "stem  |  stem\n",
      "that  |  that\n",
      "affixes  |  affix\n",
      "to  |  to\n",
      "suffixes  |  suffix\n",
      "and  |  and\n",
      "prefixes  |  prefix\n",
      ".  |  .\n"
     ]
    }
   ],
   "source": [
    "# To see another example:\n",
    "doc = nlp(\"Stemming is the process of reducing a word to its stem that affixes to suffixes and prefixes.\")\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc807983",
   "metadata": {},
   "source": [
    "### Customizing lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1e3ae9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sometimes we might want to modify the behavior of pipeline, we might want to customize it. So for this particular pipeline\n",
    "# which we loaded, we have the following components:\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e93596b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | bro\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brah\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhausted | exhaust\n"
     ]
    }
   ],
   "source": [
    "# From these components 'attribute_ruler' assigns attribute to a particular token and you can customize it. \n",
    "# In the following sentence we have words such as 'Bro', 'Brah' which gives the meaning of 'Brother'. By default the language\n",
    "# model doesn't understand these slangs. It will return themself and won't change to its base form.\n",
    "\n",
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ddec19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | Brother\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brother\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhausted | exhaust\n"
     ]
    }
   ],
   "source": [
    "# Now we want to customize the model. We know that the Bro and Brah is brother. So we can customize lemmatizer by using\n",
    "# 'attribute_ruler'.\n",
    "\n",
    "ar = nlp.get_pipe('attribute_ruler')   # Get 'attribute_ruler' component from the pipeline.\n",
    "\n",
    "ar.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\":\"Brah\"}]],{\"LEMMA\":\"Brother\"}) # Here we add custom rules for words 'Bro' and 'Brah'.\n",
    "\n",
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d4e5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So as we see now instead of words 'Bro' and 'Brah' we get 'brother'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
