{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591ae29f",
   "metadata": {},
   "source": [
    "## Text Representation (Label Encoding & One Hot Encoding)\n",
    "**Label encoding** is a technique of converting categorical data into numerical data by assigning a unique integer value to each category. In this technique, each category is mapped to a unique integer value. Label encoding is commonly used in machine learning and deep learning algorithms to represent categorical data as input features for the models. However, it should be noted that label encoding may introduce an arbitrary order or ranking among the categories, which may not be appropriate for some types of categorical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b004454",
   "metadata": {},
   "source": [
    "<img src = \"img2.jpg\" width = \"600px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400fe470",
   "metadata": {},
   "source": [
    "\n",
    "**One hot encoding** is a technique of converting categorical data into a binary vector representation. In this technique, each category is represented by a binary vector where only one bit is high (1) and all others are low (0). This vector representation is called a one-hot vector. One hot encoding is commonly used in machine learning and deep learning algorithms to represent categorical data, such as gender, color, or country, as input features for the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3489b91",
   "metadata": {},
   "source": [
    "<img src = \"img3.jpg\" width = \"600px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef3eca4",
   "metadata": {},
   "source": [
    "* Here we're going to take about **Spam detection.** So spam detection is a classical problem for text classification. So when we convert text into a vectors, it' called text representation or feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da619da2",
   "metadata": {},
   "source": [
    "<img src = \"img.jpg\" width = \"600px\" height = \"200px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80912d2d",
   "metadata": {},
   "source": [
    "After text representation we feed the vectors into a ML model. May be Naive bayes, LR, SVM ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf985050",
   "metadata": {},
   "source": [
    "<img src = \"img1.jpg\" width = \"600px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057571f",
   "metadata": {},
   "source": [
    "In **NLP** people don't use **One-Hot-Encoding** and **Label-Encoding** techniques. There are certain disadvantages of these approaches, due to those disadvantages people don't much use these approaches in ML now-a-days. In ML now they use **Word Embedding**, **TF-IDF** and **Bag Of Words (BoW).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675bbe08",
   "metadata": {},
   "source": [
    "The majore disadvantages of **Label-Encoding** and **One-Hot-Encoding** is listed bellow:\n",
    "\n",
    "   **1.** They both can't represent meaningful text representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b276944",
   "metadata": {},
   "source": [
    "<img src = \"img4.jpg\" width = \"600px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d2c425",
   "metadata": {},
   "source": [
    "**2.** Based on vocabulary (n) size we have to define n memory cells for each word. Not it wast much memory, in addition to that during ML process it consums lot of resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87efb0",
   "metadata": {},
   "source": [
    "<img src = \"img5.jpg\" width = \"600px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c668dd",
   "metadata": {},
   "source": [
    "**3.** If we have words during prediction which were not part of my vocabulay during training process, then every new word will have same numeric representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6421f234",
   "metadata": {},
   "source": [
    "<img src = \"img6.jpg\" width = \"600px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793f7a19",
   "metadata": {},
   "source": [
    "**4.** ML model expecting similar size vectors, but One-Hot-Encoding don't give the fix size lenght."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876b3f8",
   "metadata": {},
   "source": [
    "<img src = \"img7.jpg\" width = \"600px\" height = \"600px\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
