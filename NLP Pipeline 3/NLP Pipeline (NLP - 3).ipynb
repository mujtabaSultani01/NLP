{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b8e6b1",
   "metadata": {},
   "source": [
    "## NLP Pipeline\n",
    "**NLP pipeline** refers to the sequence of steps involved in natural language processing (NLP) tasks. It typically includes processes such as text preprocessing, tokenization, part-of-speech tagging, parsing, and named entity recognition. The pipeline may also involve more advanced techniques such as sentiment analysis, topic modeling, and machine translation. The goal of the NLP pipeline is to transform unstructured text data into structured data that can be analyzed and interpreted by machines.\n",
    "\n",
    "Building a real NLP application means you have to perform various steps from data aquisition and data cleaning to all the way till model model building, deployment and monitoring ... all these steps combined is called **NLP Pipeline.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40749a3",
   "metadata": {},
   "source": [
    "**Data Aquisition** is the first step in NLP Pipeline which perform by NLP team and the idea is to get necessary data required to solve a given NLP problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c129e",
   "metadata": {},
   "source": [
    "<img src = \"img.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3418f0",
   "metadata": {},
   "source": [
    "**Discarding Irrelevant Information:** As shown in image bellow, we can discard irrelevant information from each sample. For example discarding 'creator' and 'create_ts' and also we can join title with the body."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a46c22",
   "metadata": {},
   "source": [
    "<img src = \"img1.jpg\" width = \"600px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63681fb7",
   "metadata": {},
   "source": [
    "Next will be Text **Extraction and Cleanup** as shown in the bellow image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90d95d",
   "metadata": {},
   "source": [
    "<img src = \"img2.png\" width = \"600px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc8e35",
   "metadata": {},
   "source": [
    "Next in order to build an NLP model we need to split each document (text-comment) into sentences. One way to do it may be when we see dot (.) or question mark (?) we'll split it because it shows end of the sentences. This process is called **Sentence Segmentation** or **Sentence Tokenization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916ec79",
   "metadata": {},
   "source": [
    "<img src = \"img3.png\" width = \"600px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c16fe3",
   "metadata": {},
   "source": [
    "So the big deal about sentence segmentation is that we can't split sentences only based on dot(.). See the bellow image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d123a08",
   "metadata": {},
   "source": [
    "<img src = \"img4.png\" width = \"400px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69be1ad",
   "metadata": {},
   "source": [
    "So the sentence segmentation is not straight forward, we have to corporate lot of rules of the grammar of a specific language to build a good sentence splitter or sentence tokenizer. In the libraries like **NLTK** and **SpaCy** has such kind of ready math tokenizer that we can use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e189588b",
   "metadata": {},
   "source": [
    "So after sentence segmentation the next step is to create words which is called **Word Tokenization Process.** Because fro creating ML model we need words to build an NLP application. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3951626",
   "metadata": {},
   "source": [
    "<img src = \"img5.png\" width = \"400px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc28ce",
   "metadata": {},
   "source": [
    "Now let's say from a big text blog we've created sentences, from sentences we've created individual words. So after that we need some further processing. The first will be base form of words which can help us in our model accuracy, so this process is called **Stemming.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beccd7dc",
   "metadata": {},
   "source": [
    "<img src = \"img6.png\" width = \"600px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad878fe",
   "metadata": {},
   "source": [
    "So **Stemming** will not work correctly for converting all the words because it uses some simple rules. For example here it doesn't change the word 'ate' to it's base form which is 'eat', because it doesn't understand grammar. So for that we can follow other process which is called **Lemmatization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74d018",
   "metadata": {},
   "source": [
    "<img src = \"img7.png\" width = \"600px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438546d6",
   "metadata": {},
   "source": [
    "<img src = \"img8.png\" width = \"600px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e88685",
   "metadata": {},
   "source": [
    "**So** the upper whole process is called **Pre-processing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186c2f28",
   "metadata": {},
   "source": [
    "<img src = \"img9.jpg\" width = \"800px\" height = \"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e2512e",
   "metadata": {},
   "source": [
    "Now we got all these base words, but we know ML models don't understand text so we need some techniques to convert these words into numbers, I mean some meaningful conversion of words into numbers... So this process of converting words into numbers is called **Feature Enginnering.** Feature Engineering means we're extracting feature from the words. If we have a words 'better' and 'good' then there are certain feature which are similar. When we represent the words into numbers (vectors), we have different features such as, is this cricket plater? the answer will be one or zero (1,0), is this a human? (1,0). There are different techniques for **Feature Engineering** such as TF-IDF, One Hot Encoding or Word Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc973dd9",
   "metadata": {},
   "source": [
    "<img src = \"img10.png\" width = \"600px\" height = \"500px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e58b6",
   "metadata": {},
   "source": [
    "When we did **Feature Engineerin**, the next step is to build a ML model. Not all NLP applications need ML model but majority of them will need ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf5a0d",
   "metadata": {},
   "source": [
    "<img src = \"img11.png\" width = \"600px\" height = \"500px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79143fa4",
   "metadata": {},
   "source": [
    "Now to evaluate the ML model, we have something by the name of **Confustion Matrix.** On the 'X-axis' we have truth and on the 'Y-axis' we have our model prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233b60b6",
   "metadata": {},
   "source": [
    "<img src = \"img12.png\" width = \"600px\" height = \"500px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4818c75",
   "metadata": {},
   "source": [
    "So when we build a model and we evaluate the model using **GridSearchCV** or **Confusion Matrix**, then if we see the things are not good then we may again go throug preprocessing, feature engineering and model building... we keep on trying (iteration) until we find a better model for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe0676",
   "metadata": {},
   "source": [
    "<img src = \"img13.png\" width = \"800px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96402607",
   "metadata": {},
   "source": [
    "So once we find the better model for our problem, we can export it into a file and deploy it into the cloud. We can write a fast API based or Flask service around the model to serve http request and then we can deploy the service in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac3c52",
   "metadata": {},
   "source": [
    "<img src = \"img14.png\" width = \"800px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa151bc",
   "metadata": {},
   "source": [
    "We can also deploy our model to end-to-end platform such as 'datbricks', 'Amazon SageMaker' or ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed3f906",
   "metadata": {},
   "source": [
    "<img src = \"img15.png\" width = \"800px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e08665",
   "metadata": {},
   "source": [
    "Once the service is deployed, we need to **monitor and update** it periodically. Because sometimes model locally work good but when go to practical area it will not perform well, so monitoring the model is an essential task. You should make sure your model is making good prediction and you're not losing your business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f86d5",
   "metadata": {},
   "source": [
    "<img src = \"img16.png\" width = \"800px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532400df",
   "metadata": {},
   "source": [
    "**Thats were all for this notebook.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
